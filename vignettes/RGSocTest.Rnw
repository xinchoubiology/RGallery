\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{minted}
\usepackage{amsmath,amsthm, amsfonts, array}
\usepackage{graphicx,booktabs,xcolor,color,soul}
\usepackage{url}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{listings}
\definecolor{shadecolor}{rgb}{0.95,0.95,0.95}
\usepackage{hyperref}
\hypersetup{
  colorlinks = true,
  urlcolor  = blue,
  linkcolor = green,
  citecolor = red
}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\title{RGallery: A Package for 3 questions in Stochastic Average Gradient Project}
\author{Eric Xin Zhou}
\begin{document}
\maketitle
\section{3 Problems}
\begin{description}
  \item[Q1(easy)] : Use glmnet\cite{friedman2010regularization} to fit an L2-regularized logistic regression model. Use the system.time function to record how much time it takes for several data set sizes, and make a plot that shows how execution time depends on the data set size.
  \item[Q2(medium)] : create a simple R package with one function and one documentation file, and upload it to your GitHub account.
  \item[Q3(hard)] : Harder: \href{https://github.com/tdhock/when-c/blob/master/example-C-Call.org#exercise-for-the-reader}{Write an R package which uses .C to interface C code.}
\end{description}

\subsection{Data simulation setup for L2}
Given that test 1 need us provide different data set of different size to record how much time \textbf{glmnet} takes for different data size. 

I generate Gaussian data with $N$ observation and $p$ predictors. with each pair of predictors $X_j, X_{j'}$ has the same population correlation $\rho$. If $N$ and $\rho$ are determined. We generate the observed data $Y$ by adding several gaussian noise.

\begin{equation}
  Y = \sum_{j=1}^pX_j\beta_j + kZ
\end{equation}

If $Y$ is a $N \times 1$ column vector, then $X_j, X_{j'}$ are all $N \times 1$ column vectors, so $\mathbf{X}$ is a $N \times p$ matrix and $\beta$ is a $p \times 1$ column vector.

$Z$ represents noise of observation, and $k$ is chosen so that we can control signal-to-noise ratio to 3.0.

In generation model, we also should simulate the coefficient vector $\beta$, we define that 

\begin{equation}
  \beta_j = (-1)^j\exp{(\frac{-2(j-1)}{20})}
\end{equation}

This guarantee that the coefficients are constructed to have alternating signs and to be exponential descreasing.

And in logistical regression model, what we observation is $\mathcal{G}=\{1,2\}$, therefore, the logistic regression model represents the probability we observed $\{1,2\}$.

\begin{equation}
  \begin{aligned}
    Pr(G=1|x) = \frac{1}{1+e^{-(\beta_0x_0 + ... + \beta_px_p)}} \\
    Pr(G=2|x) = \frac{e^{-(\beta_0x_0 + ... + \beta_px_p)}}{1+e^{-(\beta_0x_0 + ... + \beta_px_p)}}
  \end{aligned}
\end{equation}

So we can get the column $\log{(\frac{Pr(G=1|X)}{Pr(G=2|X)})} = \sum_{j=1}^p x_j\beta_j$. In this model, if $Pr(G=1|x) > Pr(G=2|x)$, then response is actual 1, otherwise responese is 2.

However, in real observation, noise will be introduced into this regression model. As the result, we should add an $Z \sim \mathcal{N}(0,1)$ into the original LR model. 

So, the response $Y$ we generate comes from Eqn.(4)

\begin{equation}
  y = \sum_{j=1}^p X_j\beta_j + kZ
\end{equation}

And we also can tell that $y \rightarrow \beta_0x_0 + ... + \beta_px_p$, therefore, we can determine $Pr(G=1|x) = \frac{1}{1 +e^{-y}}$. And then we will define our observation $Y$ by binomial model which generate $Y$ with $Pr(Y=1)=p$ and $Pr(Y=2)=1-p$.

\section{Build a package for data set generation and time record}
Since \textbf{Q2} require us develop a package. Therefore, I develop a simple package for LR2's data simulation. And record all of the time on different data set.

And then, to record glmnet time of different data set size, therefore I create a package \href{https://github.com/xinchoubiology/RGallery}{RGallery} to solve this question. 

To solve \textbf{Q1}, if we fixed feature dimension to 300. then we execute \textbf{glmnet()} on different $N$ size data set. And we let $N \in [100, 500, 1000, 2000, 5000, 10000, 100000]$, in addition, $p$ is fixed $p = 300$.

<<glmnet-time, echo=TRUE, eval=FALSE>>=
require(RGallery)
N <- c(100, 500, 700, 1000, 1500, 2000)
glmnet_time <- sapply(N, function(n){
                          Gbenchmark(sim_dat_LR, N = n, p = 300)
                         }
                     )
colnames(glmnet_time) <- N
@

<<smallsize-t, echo=FALSE>>=
kable(data.frame(t(glmnet_time)), align = "c", col.names = c("user_time/sec", "sys_time/sec", "elapesd/sec"),booktabs=TRUE)
@

\bigskip

Even though that glmnet analysis time changed slightly with data set size increasing, function \textbf{mvrnorm()} has become too slow when data size was up to $2000 \times 300$. 

As the result, I should rewrite our \textbf{mvrnorm()} by a C++ function, so that the time consuming in data simulation can be compressed. 

\section{Rcpp accerlation}
In \textbf{Q3}, mentor give us an example of writing package by \textbf{.C} and \textbf{.Call}. And in this package, I prefer Rcpp, which provides an elegant way to handle Cpp code.

In this Rcpp source file, I apply the Choleski decomposition to execute the function \textbf{cmvrnorm()}. And then, with the help of \textbf{Rcpp}(use .Call in RcppExports.R). We can quickly caculate out $15,000 \times 300$ data set.

<<glmnetfast-time, echo=TRUE, eval=FALSE>>=
M <- c(100, 500, 700, 1000, 1500, 2000, 4000, 8000, 10000, 15000)
glmnet_time_c <- sapply(N, function(n){
                          Gbenchmark(fast_LR_sim, N = n, p = 300)
                         }
                     )
colnames(glmnet_time_c) <- M
@

<<fastsmallsize-t, echo=FALSE, eval=FALSE, fig.cap="glmnet time consuming from 100 to 15000">>=
library(knitr)
kable(data.frame(t(glmnet_time_c)), align = "c", col.names = c("user_time/sec", "sys_time/sec", "elapesd/sec"),booktabs=TRUE)
@

In addition, we also can draw a relationship map for different data set size and glmnet's elasped time.

<<glmnet_fig, echo=FALSE, eval=FALSE, fig.cap="glment time ~ data size">>=
library(ggplot2)
size2time <- data.frame(size = M, elapse = glmnet_time_c[3,])
ggplot(size2time, aes(x = size, y = elapse)) + geom_smooth(colour = "blue", method = 'lm') + geom_point(colour = "red")
@

\section{Conclusion}
Improvement next, {\color{blue}RGallery} package can be improvement, my plan is to create a cuda version for the function  \textbf{cmvrnorm}, given that matrix multiplication perform better on GPU, I wish \textbf{cudamvrnorm} could run faster than other \textbf{*mvrnorm} functions.

In this {\color{blue}RGallery} package, I have completed the three questions given on SAG project's homepage. Therefore, I think I can meet mentors requirements and I wish I could work with you for the SAG project!

\bibliography{ref}
\bibliographystyle{unsrt}
\newpage

\end{document}
